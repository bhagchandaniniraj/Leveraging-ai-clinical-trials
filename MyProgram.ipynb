{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea89cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stable-baselines3 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torch in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (4.51.3)\n",
      "Requirement already satisfied: xgboost in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (3.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: requests in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (2.32.3)\n",
      "Requirement already satisfied: tenacity in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (8.5.0)\n",
      "Requirement already satisfied: gymnasium in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (4.66.4)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (0.13.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (6.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (1.26.4)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from stable-baselines3) (3.1.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from stable-baselines3) (3.10.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from xgboost) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\niraj\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3) (3.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 18:01:46,659 - INFO - CSV file will be downloaded/saved at: d:\\IIT Patna\\Leveraging Advance AI-Clinical Trials\\clinical_trials_data.csv\n",
      "2025-06-05 18:01:46,671 - INFO - Available RAM: 3.48 GB\n",
      "2025-06-05 18:01:47,313 - WARNING - Failed to get total study count from /stats/size. Estimating via parallel fetch...\n",
      "Estimating total studies: 100%|██████████| 1000/1000 [00:12<00:00, 80.99studies/s]\n",
      "2025-06-05 18:01:59,666 - INFO - Estimated total studies: 1000\n",
      "Fetching studies:   0%|          | 0/1000 [00:00<?, ?studies/s]2025-06-05 18:01:59,676 - INFO - Available RAM: 3.53 GB\n",
      "Fetching studies:  10%|█         | 100/1000 [00:00<00:08, 107.90studies/s]2025-06-05 18:02:00,604 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies:  20%|██        | 200/1000 [00:02<00:10, 78.43studies/s] 2025-06-05 18:02:02,122 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies:  30%|███       | 300/1000 [00:03<00:09, 76.19studies/s]2025-06-05 18:02:03,482 - INFO - Available RAM: 3.50 GB\n",
      "Fetching studies:  40%|████      | 400/1000 [00:04<00:07, 80.07studies/s]2025-06-05 18:02:04,630 - INFO - Available RAM: 3.48 GB\n",
      "Fetching studies:  50%|█████     | 500/1000 [00:05<00:05, 95.44studies/s]2025-06-05 18:02:05,323 - INFO - Available RAM: 3.49 GB\n",
      "Fetching studies:  60%|██████    | 600/1000 [00:07<00:04, 86.47studies/s]2025-06-05 18:02:06,691 - INFO - Available RAM: 3.46 GB\n",
      "Fetching studies:  70%|███████   | 700/1000 [00:08<00:03, 81.14studies/s]2025-06-05 18:02:08,078 - INFO - Available RAM: 3.42 GB\n",
      "Fetching studies:  80%|████████  | 800/1000 [00:09<00:02, 88.95studies/s]2025-06-05 18:02:08,972 - INFO - Available RAM: 3.40 GB\n",
      "Fetching studies:  90%|█████████ | 900/1000 [00:10<00:01, 81.31studies/s]2025-06-05 18:02:10,432 - INFO - Available RAM: 3.41 GB\n",
      "Fetching studies: 100%|██████████| 1000/1000 [00:12<00:00, 79.92studies/s]2025-06-05 18:02:11,731 - INFO - Available RAM: 3.41 GB\n",
      "Fetching studies: 1100studies [00:13, 77.43studies/s]                     2025-06-05 18:02:13,113 - INFO - Available RAM: 3.42 GB\n",
      "Fetching studies: 1200studies [00:14, 75.79studies/s]2025-06-05 18:02:14,497 - INFO - Available RAM: 3.41 GB\n",
      "Fetching studies: 1300studies [00:16, 74.86studies/s]2025-06-05 18:02:15,872 - INFO - Available RAM: 3.50 GB\n",
      "Fetching studies: 1400studies [00:17, 75.19studies/s]2025-06-05 18:02:17,191 - INFO - Available RAM: 3.51 GB\n",
      "Fetching studies: 1500studies [00:18, 77.32studies/s]2025-06-05 18:02:18,396 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies: 1600studies [00:19, 85.13studies/s]2025-06-05 18:02:19,293 - INFO - Available RAM: 3.51 GB\n",
      "Fetching studies: 1700studies [00:20, 82.27studies/s]2025-06-05 18:02:20,606 - INFO - Available RAM: 3.51 GB\n",
      "Fetching studies: 1800studies [00:21, 98.36studies/s]2025-06-05 18:02:21,158 - INFO - Available RAM: 3.51 GB\n",
      "Fetching studies: 1900studies [00:22, 89.28studies/s]2025-06-05 18:02:22,520 - INFO - Available RAM: 3.51 GB\n",
      "Fetching studies: 2000studies [00:23, 106.16studies/s]2025-06-05 18:02:23,048 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies: 2100studies [00:24, 87.77studies/s] 2025-06-05 18:02:24,646 - INFO - Available RAM: 3.51 GB\n",
      "Fetching studies: 2200studies [00:25, 91.49studies/s]2025-06-05 18:02:25,632 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies: 2300studies [00:27, 84.03studies/s]2025-06-05 18:02:27,048 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies: 2400studies [00:28, 81.43studies/s]2025-06-05 18:02:28,364 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies: 2500studies [00:30, 77.97studies/s]2025-06-05 18:02:29,774 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies: 2600studies [00:31, 82.07studies/s]2025-06-05 18:02:30,845 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies: 2700studies [00:32, 78.28studies/s]2025-06-05 18:02:32,262 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies: 2800studies [00:34, 75.40studies/s]2025-06-05 18:02:33,697 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies: 2900studies [00:35, 75.16studies/s]2025-06-05 18:02:35,039 - INFO - Available RAM: 3.51 GB\n",
      "Fetching studies: 3000studies [00:36, 83.87studies/s]2025-06-05 18:02:35,908 - INFO - Available RAM: 3.51 GB\n",
      "Fetching studies: 3100studies [00:36, 101.09studies/s]2025-06-05 18:02:36,423 - INFO - Available RAM: 3.52 GB\n",
      "Fetching studies: 3200studies [00:38, 86.52studies/s] 2025-06-05 18:02:37,968 - INFO - Available RAM: 3.51 GB\n",
      "Fetching studies: 3300studies [00:39, 82.35studies/s]2025-06-05 18:02:39,318 - INFO - Available RAM: 3.45 GB\n",
      "Fetching studies: 3400studies [00:41, 77.41studies/s]2025-06-05 18:02:40,792 - INFO - Available RAM: 3.45 GB\n",
      "Fetching studies: 3500studies [00:42, 80.95studies/s]2025-06-05 18:02:41,894 - INFO - Available RAM: 3.47 GB\n",
      "Fetching studies: 3600studies [00:43, 79.67studies/s]2025-06-05 18:02:43,198 - INFO - Available RAM: 3.47 GB\n",
      "Fetching studies: 3700studies [00:44, 80.12studies/s]2025-06-05 18:02:44,428 - INFO - Available RAM: 3.49 GB\n",
      "Fetching studies: 3800studies [00:46, 77.65studies/s]2025-06-05 18:02:45,810 - INFO - Available RAM: 3.48 GB\n",
      "Fetching studies: 3900studies [00:46, 94.55studies/s]2025-06-05 18:02:46,329 - INFO - Available RAM: 3.48 GB\n",
      "Fetching studies: 4000studies [00:48, 85.14studies/s]2025-06-05 18:02:47,777 - INFO - Available RAM: 3.47 GB\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install stable-baselines3 torch transformers xgboost scikit-learn pandas requests tenacity gymnasium tqdm imbalanced-learn psutil numpy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from stable_baselines3 import PPO\n",
    "import pandas as pd\n",
    "import requests\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import threading\n",
    "import os\n",
    "import csv\n",
    "import multiprocessing as mp\n",
    "import logging\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import psutil\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# File to save the API data\n",
    "DATA_FILE = \"clinical_trials_data.csv\"\n",
    "# Temporary file for intermediate results\n",
    "TEMP_FILE = \"temp_data.npz\"\n",
    "# Lock for thread-safe CSV writing\n",
    "csv_lock = threading.Lock()\n",
    "\n",
    "# Display the CSV file download location\n",
    "csv_path = os.path.abspath(DATA_FILE)\n",
    "logging.info(f\"CSV file will be downloaded/saved at: {csv_path}\")\n",
    "\n",
    "# Function to check available RAM\n",
    "def check_available_ram():\n",
    "    memory = psutil.virtual_memory()\n",
    "    available_ram = memory.available / (1024 ** 3)  # Convert to GB\n",
    "    logging.info(f\"Available RAM: {available_ram:.2f} GB\")\n",
    "    return available_ram\n",
    "\n",
    "# Step 1: Fetch Total Number of Studies with Parallel Estimation and Progress Bar\n",
    "def fetch_page_for_count(page_token=None):\n",
    "    base_url = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "    params = {\n",
    "        \"pageSize\": 100,\n",
    "        \"pageToken\": page_token\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            studies = data.get('studies', [])\n",
    "            next_page_token = data.get('nextPageToken')\n",
    "            return len(studies), next_page_token\n",
    "        else:\n",
    "            logging.error(f\"Error fetching page for count: {response.status_code} - {response.text}\")\n",
    "            return 0, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception during page fetch for count: {e}\")\n",
    "        return 0, None\n",
    "\n",
    "def get_total_study_count(max_workers=1):\n",
    "    try:\n",
    "        # First, try the /stats/size endpoint\n",
    "        base_url = \"https://clinicaltrials.gov/api/v2/stats/size\"\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            total_studies = data.get('studies', 0)\n",
    "            if total_studies > 0:\n",
    "                logging.info(f\"Total studies from /stats/size: {total_studies}\")\n",
    "                return total_studies\n",
    "\n",
    "        # If /stats/size fails or returns 0, estimate by fetching pages in parallel with a progress bar\n",
    "        logging.warning(\"Failed to get total study count from /stats/size. Estimating via parallel fetch...\")\n",
    "        total_count = 0\n",
    "        next_page_token = None\n",
    "        page_tokens = [None]  # Start with the first page\n",
    "        max_pages_to_estimate = 10  # Limit estimation to 10 pages to avoid infinite loop\n",
    "\n",
    "        with tqdm(total=max_pages_to_estimate * 100, desc=\"Estimating total studies\", unit=\"studies\") as pbar:\n",
    "            pages_fetched = 0\n",
    "            while page_tokens and pages_fetched < max_pages_to_estimate:\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                    future_to_token = {executor.submit(fetch_page_for_count, token): token for token in page_tokens}\n",
    "                    page_tokens = []\n",
    "\n",
    "                    for future in concurrent.futures.as_completed(future_to_token):\n",
    "                        page_count, next_token = future.result()\n",
    "                        total_count += page_count\n",
    "                        pbar.update(page_count)\n",
    "                        if next_token:\n",
    "                            page_tokens.append(next_token)\n",
    "                pages_fetched += 1\n",
    "\n",
    "        # Estimate total based on pages fetched\n",
    "        if total_count > 0:\n",
    "            pages_fetched = total_count // 100\n",
    "            estimated_total = total_count * (max_pages_to_estimate / pages_fetched) if pages_fetched > 0 else total_count * 10\n",
    "            logging.info(f\"Estimated total studies: {int(estimated_total)}\")\n",
    "            return int(estimated_total)\n",
    "        else:\n",
    "            logging.warning(\"Could not estimate total study count. Using default large number for progress.\")\n",
    "            return 100000  # Default large number for progress bar\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception occurred while fetching total study count: {e}\")\n",
    "        return 100000  # Default large number for progress bar\n",
    "\n",
    "# Step 1.1: Count Records in CSV with Progress Bar (Memory-Efficient)\n",
    "def check_existing_csv():\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        return 0  # File doesn't exist, need to download\n",
    "\n",
    "    try:\n",
    "        total_records = 0\n",
    "        file_size = os.path.getsize(DATA_FILE) // (1024 ** 2)  # File size in MB\n",
    "        with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            with tqdm(total=file_size, desc=\"Counting CSV records\", unit=\"MB\") as pbar:\n",
    "                for _ in reader:\n",
    "                    total_records += 1\n",
    "                    # Update progress bar based on current position in file\n",
    "                    current_pos = f.tell() // (1024 ** 2)  # Current position in MB\n",
    "                    pbar.n = min(current_pos, file_size)  # Update progress\n",
    "                    pbar.refresh()\n",
    "        # Subtract 1 for the header row\n",
    "        total_records -= 1\n",
    "        logging.info(f\"Existing CSV contains {total_records} records.\")\n",
    "        return total_records\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading existing CSV: {e}. Will re-download data.\")\n",
    "        return 0\n",
    "\n",
    "# Step 1.2: Fetch a Single Page of Data\n",
    "def fetch_page(page_token=None):\n",
    "    base_url = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "    params = {\n",
    "        \"pageSize\": 100,\n",
    "        \"pageToken\": page_token\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            logging.error(f\"Error fetching page with token {page_token}: {response.status_code} - {response.text}\")\n",
    "            return None, None\n",
    "        data = response.json()\n",
    "        studies = data.get('studies', [])\n",
    "        next_page_token = data.get('nextPageToken')\n",
    "        return studies, next_page_token\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception occurred during API request for page with token {page_token}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Step 1.3: Save Fetched Studies to CSV (Thread-Safe)\n",
    "def save_to_csv(studies):\n",
    "    if not studies:\n",
    "        return\n",
    "\n",
    "    df_page = pd.json_normalize(studies)\n",
    "    with csv_lock:\n",
    "        mode = 'a' if os.path.exists(DATA_FILE) else 'w'\n",
    "        header = not os.path.exists(DATA_FILE)\n",
    "        df_page.to_csv(DATA_FILE, mode=mode, header=header, index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "# Step 1.4: Fetch All Data from ClinicalTrials.gov API with Parallel Downloads\n",
    "def fetch_clinical_trials(max_workers=1):\n",
    "    check_available_ram()\n",
    "    total_studies = get_total_study_count(max_workers=max_workers)\n",
    "    if total_studies == 0:\n",
    "        logging.warning(\"API returned 0 studies. Attempting to fetch at least one page...\")\n",
    "        studies, next_page_token = fetch_page()\n",
    "        if studies:\n",
    "            total_studies = max(len(studies) * 100, 100000)  # Rough estimate\n",
    "        else:\n",
    "            logging.error(\"No studies fetched. Using default large number for progress.\")\n",
    "            total_studies = 100000\n",
    "\n",
    "    existing_records = check_existing_csv()\n",
    "    if existing_records >= total_studies > 0:\n",
    "        logging.info(\"CSV file already contains all records. Skipping download.\")\n",
    "    else:\n",
    "        if os.path.exists(DATA_FILE):\n",
    "            os.remove(DATA_FILE)\n",
    "\n",
    "        studies_fetched = 0\n",
    "        next_page_token = None\n",
    "        page_tokens = [None]\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            with tqdm(total=total_studies, desc=\"Fetching studies\", unit=\"studies\") as pbar:\n",
    "                while page_tokens:\n",
    "                    check_available_ram()\n",
    "                    if psutil.virtual_memory().available / (1024 ** 3) < 0.5:  # Less than 0.5 GB available\n",
    "                        logging.warning(\"Low RAM available. Pausing fetch to free memory...\")\n",
    "                        sys.stdout.flush()\n",
    "                        break\n",
    "                    future_to_token = {executor.submit(fetch_page, token): token for token in page_tokens}\n",
    "                    page_tokens = []\n",
    "\n",
    "                    for future in concurrent.futures.as_completed(future_to_token):\n",
    "                        studies, next_token = future.result()\n",
    "                        if studies:\n",
    "                            save_to_csv(studies)\n",
    "                            studies_fetched += len(studies)\n",
    "                            pbar.update(len(studies))\n",
    "                        if next_token:\n",
    "                            page_tokens.append(next_token)\n",
    "\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        logging.error(\"No data was fetched. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    logging.info(f\"\\nReading fetched data from {DATA_FILE}...\")\n",
    "    df_chunks = pd.read_csv(DATA_FILE, chunksize=250, quoting=csv.QUOTE_ALL, on_bad_lines='skip')\n",
    "    df = pd.concat(df_chunks, ignore_index=True)\n",
    "    logging.info(f\"Fetched {len(df)} studies from ClinicalTrials.gov.\")\n",
    "    return df\n",
    "\n",
    "# Step 1.5: Analyze the Fetched Data with Safe Key Access\n",
    "def analyze_data(df):\n",
    "    if df.empty:\n",
    "        logging.info(\"No data to analyze.\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"\\nDataset Analysis:\")\n",
    "    logging.info(f\"Total number of studies: {len(df)}\")\n",
    "\n",
    "    # Check for studies with results\n",
    "    if 'resultsSection' in df.columns:\n",
    "        studies_with_results = df[df['resultsSection'].notna()]\n",
    "        logging.info(f\"Number of studies with results: {len(studies_with_results)}\")\n",
    "    else:\n",
    "        logging.info(\"No 'resultsSection' field found in the data.\")\n",
    "\n",
    "    if 'protocolSection.designModule.studyType' in df:\n",
    "        study_types = df['protocolSection.designModule.studyType'].value_counts()\n",
    "        logging.info(\"\\nStudy Types Distribution:\")\n",
    "        logging.info(study_types.to_string())\n",
    "    else:\n",
    "        logging.info(\"No 'protocolSection.designModule.studyType' field found in the data.\")\n",
    "\n",
    "    if 'protocolSection.designModule.phases' in df:\n",
    "        phases = df['protocolSection.designModule.phases'].explode().value_counts()\n",
    "        logging.info(\"\\nPhases Distribution:\")\n",
    "        logging.info(phases.to_string())\n",
    "    else:\n",
    "        logging.info(\"No 'protocolSection.designModule.phases' field found in the data.\")\n",
    "\n",
    "    if 'resultsSection.resultsFirstPostDate' in df:\n",
    "        results_dates = pd.to_datetime(df['resultsSection.resultsFirstPostDate'].dropna())\n",
    "        if not results_dates.empty:\n",
    "            logging.info(\"\\nResults First Posted Date Range:\")\n",
    "            logging.info(f\"Earliest: {results_dates.min()}\")\n",
    "            logging.info(f\"Latest: {results_dates.max()}\")\n",
    "    else:\n",
    "        logging.info(\"No 'resultsSection.resultsFirstPostDate' field found in the data.\")\n",
    "\n",
    "    if 'protocolSection.armsInterventionsModule.interventions' in df:\n",
    "        interventions_present = df['protocolSection.armsInterventionsModule.interventions'].notna().sum()\n",
    "        logging.info(f\"\\nNumber of studies with interventions: {interventions_present}\")\n",
    "    else:\n",
    "        logging.info(\"No 'protocolSection.armsInterventionsModule.interventions' field found in the data.\")\n",
    "\n",
    "    if 'resultsSection.outcomeMeasuresModule.outcomeMeasures' in df:\n",
    "        outcomes_present = df['resultsSection.outcomeMeasuresModule.outcomeMeasures'].notna().sum()\n",
    "        logging.info(f\"Number of studies with outcome measures: {outcomes_present}\")\n",
    "    else:\n",
    "        logging.info(\"No 'resultsSection.outcomeMeasuresModule.outcomeMeasures' field found in the data.\")\n",
    "\n",
    "# Step 1.6: Fallback Simulated Data if API Returns Insufficient Results\n",
    "def simulate_clinical_data(n_samples=1000):\n",
    "    logging.info(\"Simulating clinical trial data as fallback...\")\n",
    "    X_num = np.random.rand(n_samples, 3)  # age, sex, dosage\n",
    "    X_text = [\"Simulated summary\"] * n_samples\n",
    "    y = np.random.randint(0, 2, size=n_samples)  # ADE occurrence (0 or 1)\n",
    "    return X_num, X_text, y\n",
    "\n",
    "# Step 2: Preprocess API Data and Simulate Patient-Level Data (A) with Temporary File Saving\n",
    "def preprocess_clinical_data(df, min_samples=1000, chunk_size=100):\n",
    "    if df.empty or len(df) < 5:\n",
    "        logging.info(\"Insufficient data from API. Using simulated data.\")\n",
    "        return simulate_clinical_data(n_samples=min_samples)\n",
    "\n",
    "    # Initialize lists for chunked processing\n",
    "    numerical_features_chunks = []\n",
    "    textual_data_chunks = []\n",
    "    labels_chunks = []\n",
    "\n",
    "    chunk_numerical = []\n",
    "    chunk_textual = []\n",
    "    chunk_labels = []\n",
    "    chunk_count = 0\n",
    "\n",
    "    with tqdm(total=len(df), desc=\"Preprocessing data\", unit=\"rows\") as pbar:\n",
    "        for idx, row in df.iterrows():\n",
    "            enrollment = row.get('protocolSection.eligibilityModule.enrollmentCount', 0)\n",
    "            if enrollment == 0:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            age_group = row.get('protocolSection.eligibilityModule.minimumAge', 'ADULT, OLDER_ADULT')\n",
    "            sex = row.get('protocolSection.eligibilityModule.sex', 'ALL')\n",
    "            interventions = row.get('protocolSection.armsInterventionsModule.interventions', [{}])\n",
    "            # Safely access outcomes\n",
    "            outcomes = row.get('resultsSection.outcomeMeasuresModule.outcomeMeasures', [{}]) if 'resultsSection.outcomeMeasuresModule.outcomeMeasures' in df.columns else [{}]\n",
    "            summary = row.get('protocolSection.descriptionModule.briefSummary', \"No summary available\")\n",
    "\n",
    "            ade_prob = 0.3\n",
    "            for outcome in outcomes:\n",
    "                title = outcome.get('title', '').lower()\n",
    "                description = outcome.get('description', '').lower()\n",
    "                if 'adverse event' in title or 'safety' in title or 'adverse event' in description or 'safety' in description:\n",
    "                    ade_prob = 0.5\n",
    "                    break\n",
    "\n",
    "            dosage = 1.0\n",
    "            for intervention in interventions:\n",
    "                name = intervention.get('name', '').lower()\n",
    "                if '60mg/m2' in name:\n",
    "                    dosage = 60.0\n",
    "                elif '80mg/m2' in name:\n",
    "                    dosage = 80.0\n",
    "                elif '100mg/m2' in name:\n",
    "                    dosage = 100.0\n",
    "\n",
    "            for _ in range(int(enrollment)):\n",
    "                if 'ADULT' in age_group and 'OLDER_ADULT' in age_group:\n",
    "                    age = np.random.randint(18, 100)\n",
    "                elif 'ADULT' in age_group:\n",
    "                    age = np.random.randint(18, 65)\n",
    "                else:\n",
    "                    age = np.random.randint(65, 100)\n",
    "\n",
    "                sex_val = np.random.choice([0, 1])\n",
    "                chunk_numerical.append([age, sex_val, dosage])\n",
    "                chunk_textual.append(summary)\n",
    "                label = 1 if np.random.random() < ade_prob else 0\n",
    "                chunk_labels.append(label)\n",
    "\n",
    "                chunk_count += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "                # Save chunk to temporary file when it reaches chunk_size\n",
    "                if chunk_count >= chunk_size:\n",
    "                    numerical_features_chunks.append(np.array(chunk_numerical))\n",
    "                    textual_data_chunks.append(chunk_textual)\n",
    "                    labels_chunks.append(np.array(chunk_labels))\n",
    "                    # Save to temporary file\n",
    "                    with open(TEMP_FILE, 'ab') as f:\n",
    "                        np.savez(f, numerical=np.array(chunk_numerical), textual=np.array(chunk_textual, dtype=object), labels=np.array(chunk_labels))\n",
    "                    chunk_numerical = []\n",
    "                    chunk_textual = []\n",
    "                    chunk_labels = []\n",
    "                    chunk_count = 0\n",
    "                    check_available_ram()\n",
    "\n",
    "    # Save any remaining data\n",
    "    if chunk_numerical:\n",
    "        numerical_features_chunks.append(np.array(chunk_numerical))\n",
    "        textual_data_chunks.append(chunk_textual)\n",
    "        labels_chunks.append(np.array(chunk_labels))\n",
    "        with open(TEMP_FILE, 'ab') as f:\n",
    "            np.savez(f, numerical=np.array(chunk_numerical), textual=np.array(chunk_textual, dtype=object), labels=np.array(chunk_labels))\n",
    "\n",
    "    if len(numerical_features_chunks) == 0:\n",
    "        numerical_features = np.array([])\n",
    "        textual_data = []\n",
    "        labels = np.array([])\n",
    "    else:\n",
    "        numerical_features = np.vstack(numerical_features_chunks)\n",
    "        textual_data = [item for sublist in textual_data_chunks for item in sublist]\n",
    "        labels = np.concatenate(labels_chunks)\n",
    "\n",
    "    if len(numerical_features) < min_samples:\n",
    "        logging.info(f\"Only {len(numerical_features)} samples generated. Supplementing with simulated data.\")\n",
    "        X_num_sim, X_text_sim, y_sim = simulate_clinical_data(n_samples=min_samples - len(numerical_features))\n",
    "        numerical_features = np.vstack([numerical_features, X_num_sim]) if len(numerical_features) > 0 else X_num_sim\n",
    "        textual_data.extend(X_text_sim)\n",
    "        labels = np.concatenate([labels, y_sim]) if len(labels) > 0 else y_sim\n",
    "\n",
    "    return numerical_features, textual_data, labels\n",
    "\n",
    "# Step 3: Extract Textual Embeddings Using BERT with Chunked Processing\n",
    "def get_bert_embeddings(texts, chunk_size=50):\n",
    "    try:\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        model.eval()\n",
    "\n",
    "        embeddings = []\n",
    "        with tqdm(total=len(texts), desc=\"Extracting BERT embeddings\", unit=\"texts\") as pbar:\n",
    "            for i in range(0, len(texts), chunk_size):\n",
    "                chunk = texts[i:i + chunk_size]\n",
    "                inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding=True, max_length=128, return_attention_mask=True)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                chunk_embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "                embeddings.append(chunk_embeddings)\n",
    "                pbar.update(len(chunk))\n",
    "                check_available_ram()\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in BERT embeddings: {e}. Using random embeddings as fallback.\")\n",
    "        return np.random.rand(len(texts), 768)\n",
    "\n",
    "# Step 4: Define DNN for ADE Prediction (B --> G: LLMs) with Improved Architecture\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Step 5: Train DNN with Improved Hyperparameters\n",
    "def train_dnn(X, y, epochs=200):\n",
    "    try:\n",
    "        model = DNN(input_dim=X.shape[1])\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-3)\n",
    "        X_tensor = torch.FloatTensor(X).float()\n",
    "        y_tensor = torch.FloatTensor(y).float().view(-1, 1)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return model, outputs.detach().numpy().flatten()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in DNN training: {e}. Returning random predictions.\")\n",
    "        return None, np.random.rand(len(y))\n",
    "\n",
    "# Step 6: Gradient Boosting with XGBoost (B --> H: AI Models) with Improved Hyperparameters\n",
    "def train_gbm(X, y):\n",
    "    try:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "        model = xgb.XGBClassifier(max_depth=5, learning_rate=0.05, n_estimators=200, random_state=42)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        y_gbm_pred = model.predict_proba(X_scaled)[:, 1]\n",
    "        return y_gbm_pred\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in GBM training: {e}. Returning random predictions.\")\n",
    "        return np.random.rand(len(y))\n",
    "\n",
    "# Step 7: Ensemble Prediction (C: ADE Prediction & Prevention)\n",
    "def ensemble_predict(y_dnn, y_gbm, w=0.6):\n",
    "    return w * y_dnn + (1 - w) * y_gbm\n",
    "\n",
    "# Step 8: Reinforcement Learning for Treatment Adaptation (C --> J)\n",
    "class TrialEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(TrialEnv, self).__init__()\n",
    "        self.protocol = 1.0\n",
    "        self.state = 0.0\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 10\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        self.protocol += action[0]\n",
    "        reward = -np.random.random() if action[0] > 0 else -1\n",
    "        self.state = np.array([self.protocol], dtype=np.float32)\n",
    "        done = self.step_count >= self.max_steps\n",
    "        truncated = False\n",
    "        return self.state, reward, done, truncated, {}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count = 0\n",
    "        self.state = np.array([0.0], dtype=np.float32)\n",
    "        self.protocol = 1.0\n",
    "        return self.state, {}\n",
    "\n",
    "def train_rl():\n",
    "    try:\n",
    "        env = TrialEnv()\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=0, learning_rate=0.0003)\n",
    "        model.learn(total_timesteps=2000)\n",
    "        return model, env\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in RL training: {e}. Returning dummy model.\")\n",
    "        env = TrialEnv()\n",
    "        class DummyModel:\n",
    "            def __init__(self):\n",
    "                self.env = env\n",
    "        return DummyModel(), env\n",
    "\n",
    "# Step 9: Evaluate Model (Step 6: Metrics)\n",
    "def evaluate_model(y_true, y_pred_prob):\n",
    "    try:\n",
    "        y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred_binary)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_pred_prob)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        return f1, roc_auc, pr_auc\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in evaluation: {e}. Returning default metrics.\")\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "# Main Workflow\n",
    "def main():\n",
    "    df = fetch_clinical_trials()\n",
    "    analyze_data(df)\n",
    "    X_num, X_text, y = preprocess_clinical_data(df)\n",
    "\n",
    "    if len(X_num) == 0:\n",
    "        logging.error(\"No data available to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    X_text_emb = get_bert_embeddings(X_text)\n",
    "    X = np.hstack((X_num, X_text_emb))\n",
    "\n",
    "    eligibility_criteria = X_num[:, 0].mean() if X_num.shape[0] > 0 else 0.0\n",
    "    treatment_protocols = X_num[:, 2].sum() if X_num.shape[0] > 0 else 0.0\n",
    "\n",
    "    dnn_model, y_dnn = train_dnn(X, y)\n",
    "    y_gbm = train_gbm(X, y)\n",
    "    y_pred = ensemble_predict(y_dnn, y_gbm)\n",
    "\n",
    "    f1, roc_auc, pr_auc = evaluate_model(y, y_pred)\n",
    "    logging.info(f\"Evaluation Metrics:\\nF1 Score: {f1:.2f}\\nROC-AUC: {roc_auc:.2f}\\nPR-AUC: {pr_auc:.2f}\\n\")\n",
    "\n",
    "    rl_model, env = train_rl()\n",
    "    protocol_value = env.protocol\n",
    "    personalized_medicine = f\"Protocol adjusted to: {protocol_value:.2f}\"\n",
    "    reduced_costs = f\"Trial costs reduced by optimizing with F1: {f1:.2f}\"\n",
    "\n",
    "    logging.info(f\"Eligibility Criteria (Average Age): {eligibility_criteria:.2f}\")\n",
    "    logging.info(f\"Treatment Protocols (Total Dosage): {treatment_protocols:.2f}\")\n",
    "    logging.info(f\"Personalized Medicine: {personalized_medicine}\")\n",
    "    logging.info(f\"Reduced Trial Costs: {reduced_costs}\")\n",
    "\n",
    "    # Delete temporary file\n",
    "    if os.path.exists(TEMP_FILE):\n",
    "        os.remove(TEMP_FILE)\n",
    "        logging.info(f\"Deleted temporary file: {TEMP_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
