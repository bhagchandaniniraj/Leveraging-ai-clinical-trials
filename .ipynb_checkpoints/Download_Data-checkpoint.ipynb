{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fa8b7-8ce9-4cc9-bb07-263b9cdf205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from json import JSONDecodeError\n",
    "import sqlite3\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from queue import Queue\n",
    "from threading import Lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aab129e4-0a57-429b-a7df-7170857f5255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting download of all clinical trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pages : 5415page [2:55:34,  1.95s/page]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Download complete. Total studies saved: 541402\n",
      "Output file: clinical_trials_full.json\n",
      "Could not load output file for verification: \n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://clinicaltrials.gov/api/v2/studies\"\n",
    "PAGE_SIZE = 100  # API max is 100\n",
    "OUTPUT_FILE = \"clinical_trials_full.json\"\n",
    "SLEEP_BETWEEN_REQUESTS = 0.5  # seconds\n",
    "\n",
    "def fetch_all_studies():\n",
    "    \"\"\"Fetch all studies from ClinicalTrials.gov API v2 and write to a JSON file.\"\"\"\n",
    "    print(\"ðŸš€ Starting download of all clinical trials...\")\n",
    "    next_token = None\n",
    "    total_fetched = 0\n",
    "    first_record = True\n",
    "\n",
    "    # Open output file and write opening bracket for JSON array\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        outfile.write(\"[\\n\")\n",
    "\n",
    "        with tqdm(desc=\"Downloading pages \", unit=\" page\") as pbar:\n",
    "            while True:\n",
    "                params = {\"pageSize\": PAGE_SIZE}\n",
    "                if next_token:\n",
    "                    params[\"pageToken\"] = next_token\n",
    "\n",
    "                try:\n",
    "                    response = requests.get(BASE_URL, params=params, timeout=30)\n",
    "                    response.raise_for_status()\n",
    "                    data = response.json()\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nâš ï¸ Error fetching page: {e}\")\n",
    "                    print(\"Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "\n",
    "                studies = data.get(\"studies\", [])\n",
    "                if not studies:\n",
    "                    break\n",
    "\n",
    "                for study in studies:\n",
    "                    # Write comma before every record except the first\n",
    "                    if not first_record:\n",
    "                        outfile.write(\",\\n\")\n",
    "                    json.dump(study, outfile, ensure_ascii=False)\n",
    "                    first_record = False\n",
    "                    total_fetched += 1\n",
    "\n",
    "                next_token = data.get(\"nextPageToken\")\n",
    "                pbar.update(1)\n",
    "\n",
    "                if not next_token:\n",
    "                    break\n",
    "\n",
    "                time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "        # Close JSON array\n",
    "        outfile.write(\"\\n]\")\n",
    "\n",
    "    print(f\"\\nâœ… Download complete. Total studies saved: {total_fetched}\")\n",
    "    print(f\"Output file: {OUTPUT_FILE}\")\n",
    "\n",
    "    # Show the first two records for verification\n",
    "    try:\n",
    "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_data = json.load(f)\n",
    "            print(f\"\\nFirst two records:\")\n",
    "            print(json.dumps(all_data[:2], indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load output file for verification: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_all_studies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25779a51-0bc8-46e6-a5c2-49a4de0a3071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… File can be opened for reading.\n",
      "âŒ Could not open or parse clinical_trials_full.json: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "filepath = \"clinical_trials_full.json\"\n",
    "\n",
    "# Step 1: File existence\n",
    "if not os.path.isfile(filepath):\n",
    "    print(f\"âŒ File not found: {filepath}\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: File permissions\n",
    "try:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        f.read(1)\n",
    "    print(\"âœ… File can be opened for reading.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ File cannot be opened: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Step 3: Try JSON validation\n",
    "try:\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        json.load(f)\n",
    "    print(f\"âœ… {filepath} is valid JSON.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"âŒ JSON syntax error in {filepath}:\")\n",
    "    print(f\"   Line {e.lineno}, Column {e.colno}: {e.msg}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Could not open or parse {filepath}: {e}\")\n",
    "\n",
    "import ijson\n",
    "\n",
    "filepath = \"clinical_trials_full.json\"\n",
    "record_count = 0\n",
    "error_count = 0\n",
    "\n",
    "with open(filepath, 'rb') as f:\n",
    "    try:\n",
    "        for record in ijson.items(f, 'item'):\n",
    "            record_count += 1\n",
    "            # Optionally, add further validation here\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error while parsing record {record_count+1}: {e}\")\n",
    "        error_count += 1\n",
    "\n",
    "print(f\"\\nâœ… Finished streaming validation.\")\n",
    "print(f\"Total records parsed: {record_count}\")\n",
    "print(f\"Total errors encountered: {error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e48427b8-03ea-4268-abca-6b507248ebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Could not open or parse clinical_trials_full.json: \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def validate_json_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            json.load(f)\n",
    "        print(f\"âœ… {filepath} is valid JSON.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ JSON syntax error in {filepath}:\")\n",
    "        print(f\"   Line {e.lineno}, Column {e.colno}: {e.msg}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not open or parse {filepath}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    validate_json_file(\"clinical_trials_full.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee0f3bc4-ef87-4ed6-8fe4-a7adb696be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "File size: 10149045454\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"clinical_trials_full.json\"))\n",
    "\n",
    "import os\n",
    "print(\"File size:\", os.path.getsize(\"clinical_trials_full.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeca13c-3830-4a0c-a752-2e2bdb4279e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not to be ran ...\n",
    "\n",
    "import json\n",
    "import os\n",
    "from json import JSONDecodeError\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_large_json(input_file, output_file):\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"Input file '{input_file}' not found\")\n",
    "\n",
    "    file_size = os.path.getsize(input_file)\n",
    "    valid_records = 0\n",
    "    total_errors = 0\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as outfile, \\\n",
    "         tqdm(total=file_size, unit='B', unit_scale=True, desc=\"Cleaning data\") as pbar:\n",
    "\n",
    "        outfile.write('[\\n')\n",
    "        buffer = ''\n",
    "        first_record = True\n",
    "\n",
    "        while True:\n",
    "            chunk = infile.read(4096)  # 4KB chunks\n",
    "            if not chunk:\n",
    "                break\n",
    "            \n",
    "            buffer += chunk\n",
    "            pbar.update(len(chunk))\n",
    "            \n",
    "            # Process complete JSON objects in buffer\n",
    "            while True:\n",
    "                # Find potential JSON object start/end\n",
    "                start = buffer.find('{')\n",
    "                end = buffer.find('}', start) + 1  # Include closing brace\n",
    "\n",
    "                if start == -1 or end == 0:\n",
    "                    break  # No complete objects found\n",
    "\n",
    "                json_str = buffer[start:end]\n",
    "                buffer = buffer[end:]  # Remove processed part\n",
    "\n",
    "                try:\n",
    "                    obj = json.loads(json_str)\n",
    "                    if isinstance(obj, dict) and 'protocolSection' in obj:\n",
    "                        if not first_record:\n",
    "                            outfile.write(',\\n')\n",
    "                        json.dump(obj, outfile, ensure_ascii=False)\n",
    "                        first_record = False\n",
    "                        valid_records += 1\n",
    "                    else:\n",
    "                        total_errors += 1\n",
    "                except JSONDecodeError:\n",
    "                    total_errors += 1\n",
    "\n",
    "        outfile.write('\\n]')\n",
    "\n",
    "    print(f\"\\nCleaning complete. Valid records: {valid_records}, Errors: {total_errors}\")\n",
    "    return output_file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"clinical_trials_full.json\"\n",
    "    output_filename = \"cleaned_data.json\"\n",
    "\n",
    "    try:\n",
    "        print(\"Starting data cleaning process...\")\n",
    "        result_file = clean_large_json(input_filename, output_filename)\n",
    "        print(f\"\\nCleaned data saved to {result_file}\")\n",
    "\n",
    "        # Verify and print first two records\n",
    "        if os.path.getsize(output_filename) > 2:\n",
    "            with open(output_filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"\\nTotal cleaned records: {len(data)}\")\n",
    "                print(\"\\nFirst two valid records:\")\n",
    "                print(json.dumps(data[:2], indent=2))\n",
    "        else:\n",
    "            print(\"\\nNo valid records found in output file\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "463707bd-32b1-43f3-8e10-08072e63f874",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m count\n\u001b[0;32m     14\u001b[0m input_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclinical_trials_full.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 15\u001b[0m record_count \u001b[38;5;241m=\u001b[39m \u001b[43mcount_records_with_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal records in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecord_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m, in \u001b[0;36mcount_records_with_progress\u001b[1;34m(input_file)\u001b[0m\n\u001b[0;32m      7\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 9\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(data, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCounting records\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def count_records_with_progress(input_file):\n",
    "    file_size = os.path.getsize(input_file)\n",
    "    count = 0\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        for _ in tqdm(data, desc='Counting records'):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "input_filename = 'clinical_trials_full.json'\n",
    "record_count = count_records_with_progress(input_filename)\n",
    "print(f\"\\nTotal records in {input_filename}: {record_count}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
